{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPBD Assignment 2\n",
    "\n",
    "This notebook contains the code developed to implement the propoused solutions to this course assignment\n",
    "\n",
    "Developed by:\n",
    "    * Lucas Fischer, nº54659\n",
    "    * Joana Martins, nº54707\n",
    "    \n",
    "    \n",
    "# IMPORT NOTE:\n",
    "\n",
    "The implemented code runs locally using the sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the dataset\n",
    "\n",
    "The dataset can be obtain by running the command bellow, or by uploading the file manualy to the work directory if the file is already present in your machine\n",
    "\n",
    "# TODO ver como é o link para obter os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o sorted_data.csv https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2018-01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating result folders\n",
    "\n",
    "This code removes (if already created) and creates new result folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf spark_rdd_results && mkdir spark_rdd_results\n",
    "!cd spark_rdd_results && mkdir query3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up dependencies\n",
    "\n",
    "The first task we must complete is setting up the right dependencies for our solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark import SparkContext\n",
    "import traceback\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import calendar\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('uberfy').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "filename = \"./data/sorted_data.csv\" #TODO update this with the file location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions\n",
    "\n",
    "Functions created to help in the development of this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row(line):\n",
    "    \"\"\"\n",
    "        Function that creates a structured tuple representing a row in a RDD\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Rerturns:\n",
    "            A Strcutured tuple with 14 positions\n",
    "    \"\"\"\n",
    "    #Field - Array_position\n",
    "\n",
    "    #pickup_dt - 0      fare_amount - 8\n",
    "    #dropoff_dt - 1     tip_amount - 9\n",
    "    #trip_time - 2      total_amount - 10\n",
    "    #trip_distance - 3  pickup_cell - 11\n",
    "    #pickup_long - 4    dropoff_cell - 12\n",
    "    #pickup_lat - 5     taxi_id = 13\n",
    "    #dropoff_long - 6\n",
    "    #dropoff_lat - 7\n",
    "    \n",
    "    splitted_line = line.split(',')\n",
    "    return (\n",
    "        splitted_line[2], splitted_line[3], int(splitted_line[4]), float(splitted_line[5]), float(splitted_line[6]), \\\n",
    "        float(splitted_line[7]), float(splitted_line[8]), float(splitted_line[9]), float(splitted_line[11]), \\\n",
    "        float(splitted_line[14]), float(splitted_line[16]), estimate_cellid(float(splitted_line[7]), float(splitted_line[6])),\\\n",
    "        estimate_cellid(float(splitted_line[9]), float(splitted_line[8])), splitted_line[0]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_row_df(line):\n",
    "    \"\"\"\n",
    "        Function that creates a Structured Row object representing a Row in a DataFrame\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Returns:\n",
    "            A Row object representing a row in a Dataframe\n",
    "    \"\"\"\n",
    "    #Field - Array_position\n",
    "\n",
    "    #pickup_dt - 0      fare_amount - 8\n",
    "    #dropoff_dt - 1     tip_amount - 9\n",
    "    #trip_time - 2      total_amount - 10\n",
    "    #trip_distance - 3  pickup_cell - 11\n",
    "    #pickup_long - 4    dropoff_cell - 12\n",
    "    #pickup_lat - 5     taxi_id = 13\n",
    "    #dropoff_long - 6\n",
    "    #dropoff_lat - 7\n",
    "    \n",
    "    splitted_line = line.split(',')\n",
    "    return Row(\n",
    "        pickup_dt = splitted_line[2], dropoff_dt = splitted_line[3], trip_time = int(splitted_line[4]), \\\n",
    "        trip_distance = float(splitted_line[5]), pickup_long = float(splitted_line[6]), pickup_lat = float(splitted_line[7]), \\\n",
    "        dropoff_long = float(splitted_line[8]), dropoff_lat = float(splitted_line[9]), fare_amount = float(splitted_line[11]), \\\n",
    "        tip_amount = float(splitted_line[14]), total_amount = float(splitted_line[16]), pickup_cell = estimate_cellid(float(splitted_line[7]), float(splitted_line[6])), \\\n",
    "        dropoff_cell = estimate_cellid(float(splitted_line[9]), float(splitted_line[8])), taxi_id = splitted_line[0]\n",
    "        )   \n",
    "\n",
    "\n",
    "def filter_lines(line):\n",
    "    \"\"\"\n",
    "        Function that filters out empty lines as well as lines that have coordinates as 0.0000 (non relevant points)\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Returns:\n",
    "            True if the line passed this condition, False otherwise\n",
    "    \"\"\"\n",
    "    splitted_line = line.split(',')\n",
    "\n",
    "    lon_min = -74.916578\n",
    "    lon_max = -73.120784\n",
    "    lat_min = 40.129716\n",
    "    lat_max = 41.477183\n",
    "\n",
    "    return (\n",
    "        len(line) > 0) and \\\n",
    "        (float(splitted_line[6]) != 0) and \\\n",
    "        (float(splitted_line[8]) != 0 and \\\n",
    "        (float(splitted_line[6]) >= lon_min) and \\\n",
    "        (float(splitted_line[6]) <= lon_max) and \\\n",
    "        (float(splitted_line[7]) >= lat_min) and \\\n",
    "        (float(splitted_line[7]) <= lat_max) and \\\n",
    "        (float(splitted_line[8]) >= lon_min) and \\\n",
    "        (float(splitted_line[8]) <= lon_max) and \\\n",
    "        (float(splitted_line[9]) >= lat_min) and \\\n",
    "        (float(splitted_line[9]) <= lat_max)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def estimate_cellid(lat, lon):\n",
    "    \"\"\"\n",
    "        Function that estimates a cell ID given a latitude and longitude based on the coordinates of cell 1.1\n",
    "\n",
    "        Params:\n",
    "            lat - Input latitude for which to find the cellID\n",
    "            lon - Input longitude for which to fin the cellID\n",
    "\n",
    "        Returns:\n",
    "            A String such as 'xxx.xxx' representing the ID of the cell\n",
    "    \"\"\"\n",
    "    x0 = -74.913585 #longitude of cell 1.1\n",
    "    y0 = 41.474937  #latitude of cell 1.1\n",
    "    s = 500 #500 meters\n",
    "\n",
    "    delta_x = 0.005986 / 500.0  #Change in longitude coordinates per meter\n",
    "    delta_y = 0.004491556 /500.0    #Change in latitude coordinates per meter\n",
    "\n",
    "    cell_x = 1 + math.floor((1/2) + (lon - x0)/(s * delta_x))\n",
    "    cell_y = 1 + math.floor((1/2) + (y0 - lat)/(s * delta_y))\n",
    "    \n",
    "    return f\"{cell_x}.{cell_y}\"\n",
    "\n",
    "\n",
    "\n",
    "def create_key_value(structured_tuple):\n",
    "    \"\"\"\n",
    "        Function that from a structured tuple organizes it into a Key-Value formation.\n",
    "        The key is a tuple containing both the weekday and the hour.\n",
    "        The value is a dictionary containing only one item, this dictionary is to be merged on the reducer.\n",
    "\n",
    "        Params:\n",
    "            structured_tuple - A tuple representing a line of the input file\n",
    "\n",
    "        Returns:\n",
    "            A tuple organized into a Key-Value formation\n",
    "    \"\"\"\n",
    "\n",
    "    weekday = convert_to_weekday(structured_tuple[0])\n",
    "    hour = convert_to_hour(structured_tuple[0])\n",
    "    route = f\"{structured_tuple[11]}-{structured_tuple[12]}\"\n",
    "\n",
    "    return ((weekday, hour), {route: 1})\n",
    "\n",
    "\n",
    "\n",
    "def custom_reducer(accum, elem):\n",
    "    \"\"\"\n",
    "        Custom function to be used in reduceByKey.\n",
    "        This function well merge dictionaries counting the number of times each time appears\n",
    "\n",
    "        Params:\n",
    "            accum - An accumulator dictionary\n",
    "            elem - The dictionary of the current iteration\n",
    "\n",
    "        Returns:\n",
    "            The accumulator dictionary updated with information obtained by elem\n",
    "    \"\"\"\n",
    "\n",
    "    #store the only existing item inside elem\n",
    "    key, value = elem.popitem()\n",
    "    \n",
    "    if(key in accum): #If accum already has this key, then update its value\n",
    "        accum[key] += value\n",
    "    else:   #If accum does not have this key, add it\n",
    "        accum[key] = value\n",
    "\n",
    "    return accum\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_weekday(date):\n",
    "    \"\"\"\n",
    "        Function that converts a date to weekday\n",
    "\n",
    "        Params:\n",
    "            date - Unix timestamp formatted date in string form\n",
    "\n",
    "        Returns:\n",
    "            A string with the weekday of the input date\n",
    "    \"\"\"\n",
    "    date_obj = dt.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return (calendar.day_name[date_obj.weekday()]).lower()\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_hour(date):\n",
    "    \"\"\"\n",
    "        Function that gets the hour from a date\n",
    "\n",
    "        Params:\n",
    "            date - Unix timestamp formatted date in string form\n",
    "\n",
    "        Returns:\n",
    "            The hour portion of the input date\n",
    "    \"\"\"\n",
    "    return date[11:13]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 1\n",
    "\n",
    "In this first query the objective was to obtain an index where it would be possible to check the 10 most frequent taxi routes on any given weekday and hour.\n",
    "\n",
    "To do so the group first had to map the latitude and longitude coordinates of every event into a 500m x 500m cell in order to represent a small area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('tuesday', '10'), ['157.159-155.162', '158.163-155.162', '157.162-155.162', '158.161-155.162', '156.160-155.162', '161.157-155.162', '157.159-175.157', '157.164-155.162', '158.162-155.162', '154.160-154.162'])\n",
      "(('tuesday', '20'), ['155.162-157.163', '155.162-157.162', '156.161-158.161', '156.161-155.162', '156.166-157.164', '155.166-155.163', '161.157-160.160', '156.161-157.162', '159.160-159.160', '157.160-155.162'])\n",
      "Execution time 18.368937 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        \n",
    "    #timestamp to mesure the time taken\n",
    "    time_before = dt.now()\n",
    "\n",
    "    #read csv file (change this to the full dataset instead of just the sample)\n",
    "    raw_data = sc.textFile(filename)\n",
    "\n",
    "    #Filtering out non empty lines and lines that have a pick up or drop off coordinates as 0\n",
    "    #Also filtering lines that have coordinates that would be mapped to cells with ID greater than 300 and lower de 1\n",
    "    #These lines are considerer outliers (stated in http://debs.org/debs-2015-grand-challenge-taxi-trips/)\n",
    "    non_empty_lines = raw_data.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    #Shaping the rdd rows\n",
    "    fields = non_empty_lines.map(lambda line : create_row(line))\n",
    "\n",
    "    # ((weekday, hour), {route})\n",
    "    organized_lines = fields.map(lambda line : create_key_value(line))\n",
    "\n",
    "    #Group all values by its key, reducing them acording to custom_reducer\n",
    "    grouped = organized_lines.reduceByKey(lambda accum, elem: custom_reducer(accum, elem))\n",
    "\n",
    "    #Sort descendingly the dictionaries present in the values and take only the first 10 elements\n",
    "    top_routes = grouped.mapValues(lambda route_dict: sorted(route_dict, key = route_dict.get, reverse = True)[:10])\n",
    "\n",
    "    #Store the retrieved results\n",
    "    top_routes.saveAsTextFile(\"spark_rdd_results/query1\")\n",
    "\n",
    "    for a in top_routes.take(2):\n",
    "        print(a)\n",
    "\n",
    "    time_after = dt.now()\n",
    "    seconds = (time_after - time_before).total_seconds()\n",
    "    print(\"Execution time {} seconds\".format(seconds))\n",
    "    # sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    # sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 2\n",
    "\n",
    "Another question of interesent is \"What are the most profitable areas?\". In order to obtain a conclusion the group started by calculating the most profitable areas in a given weekday and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    #timestamp to mesure the time taken\n",
    "    time_before = dt.now()\n",
    "\n",
    "    # convert_to_weekday_udf = udf(lambda pickup_date: convert_to_weekday(pickup_date), StringType())\n",
    "    convert_to_weekday_udf = udf(lambda pickup_date: convert_to_weekday(pickup_date), StringType())\n",
    "    convert_to_hour_udf = udf(lambda pickup_date: pickup_date[11:13], StringType())\n",
    "\n",
    "    #read csv file (change this to the full dataset instead of just the sample)\n",
    "    raw_data = sc.textFile(filename)\n",
    "\n",
    "    #Filtering out non empty lines and lines that have a pick up or drop off coordinates as 0\n",
    "    non_empty_lines = raw_data.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    #Shapping the rdd rows\n",
    "    fields = non_empty_lines.map(lambda line : create_row_df(line))\n",
    "\n",
    "    #Creating DataFrame\n",
    "    lines_df = spark.createDataFrame(fields)\n",
    "\n",
    "    # Get the dropoffs of the last 15 minutes for each cell\n",
    "    # get the average of the fare\n",
    "    profit_by_area_15min = lines_df \\\n",
    "        .groupBy(window(\"dropoff_dt\", \"900 seconds\"), convert_to_weekday_udf(\"pickup_dt\").alias(\"weekday\"), convert_to_hour_udf(\"pickup_dt\").alias(\"hour\"), \"pickup_cell\") \\\n",
    "        .agg(avg(lines_df.fare_amount + lines_df.tip_amount).alias(\"median_fare\")) \\\n",
    "        .orderBy(\"median_fare\", ascending = False) \\\n",
    "        .select(\"weekday\", \"hour\", \"pickup_cell\")\n",
    "\n",
    "\n",
    "    # empty_taxis = lines_df \\\n",
    "    #     .groupBy(window(\"dropoff_dt\", \"900 seconds\"), \"dropoff_cell\") \\\n",
    "    #     .agg(countDistinct(\"taxi_id\").alias(\"empty_taxis\")) \\\n",
    "    #     .select(\"dropoff_cell\", \"empty_taxis\")\n",
    "\n",
    "    profit_by_area_15min.show(2)\n",
    "    \n",
    "    profit_by_area_15min.rdd.map(lambda row: ((row.weekday, row.hour), row.pickup_cell)).saveAsTextFile(\"spark_rdd_results/query2\")\n",
    "    \n",
    "    time_after = dt.now()\n",
    "    seconds = (time_after - time_before).total_seconds()\n",
    "    print(\"Execution time {} seconds\".format(seconds))\n",
    "\n",
    "    # sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    # sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 3\n",
    "\n",
    "Falar aqui sobre a query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[ 40.72606307 -73.99603147]\n",
      "[ 40.78036605 -73.96005749]\n",
      "[ 40.64674545 -73.78507649]\n",
      "[ 40.76848466 -73.87468834]\n",
      "[ 40.75604783 -73.98210939]\n",
      "5 -> SSE: 607.6047854043499\n",
      "Cluster Centers: \n",
      "[ 40.74337146 -73.98152608]\n",
      "[ 40.77235547 -73.86963166]\n",
      "[ 40.64553303 -73.78437704]\n",
      "[ 40.80018064 -73.94307785]\n",
      "[ 40.71168096 -74.01065348]\n",
      "[ 40.80106031 -73.9652392 ]\n",
      "[ 40.7239783  -74.00040032]\n",
      "[ 40.76015136 -73.98518381]\n",
      "[ 40.73898286 -74.00365672]\n",
      "[ 40.76093345 -73.96967685]\n",
      "[ 40.62374858 -73.97757702]\n",
      "[ 40.76927986 -73.95949391]\n",
      "[ 40.69030608 -73.99218571]\n",
      "[ 40.78541415 -73.97591658]\n",
      "[ 40.77984761 -73.95304893]\n",
      "[ 41.1581104  -74.72800269]\n",
      "[ 40.70848852 -73.79849706]\n",
      "[ 40.73882672 -73.99134331]\n",
      "[ 40.75295661 -73.97573484]\n",
      "[ 40.67905306 -73.97448264]\n",
      "[ 40.71432898 -73.95362458]\n",
      "[ 40.8392866  -73.93688329]\n",
      "[ 40.73879219 -74.54635698]\n",
      "[ 41.09031822 -73.84855259]\n",
      "[ 40.64701287 -74.17645513]\n",
      "[ 40.75254434 -73.99383228]\n",
      "[ 40.75578549 -73.92308828]\n",
      "[ 40.77180636 -73.98355095]\n",
      "[ 40.72504651 -73.86077267]\n",
      "[ 40.7265668  -73.98657862]\n",
      "[ 40.85742214 -74.26882683]\n",
      "31 -> SSE: 111.20544630784511\n",
      "Cluster Centers: \n",
      "[ 40.72091897 -73.98699478]\n",
      "[ 40.78003876 -73.95653284]\n",
      "[ 40.64579774 -73.78952305]\n",
      "[ 40.75112922 -73.97662667]\n",
      "[ 40.76958355 -73.86385327]\n",
      "[ 40.84861536 -73.93148405]\n",
      "[ 40.7445821  -73.91850791]\n",
      "[ 40.74344301 -73.99152508]\n",
      "[ 41.2160038  -74.71351013]\n",
      "[ 40.69393298 -73.99042618]\n",
      "[ 40.80321195 -73.95343106]\n",
      "[ 40.76732837 -73.95590152]\n",
      "[ 40.71292675 -74.01389807]\n",
      "[ 40.77408596 -73.98373582]\n",
      "[ 40.98901078 -73.6814641 ]\n",
      "[ 40.7582946  -73.98834086]\n",
      "[ 40.72092707 -74.00664593]\n",
      "[ 40.73334886 -73.9912019 ]\n",
      "[ 40.70671063 -73.93689352]\n",
      "[ 40.77493434 -73.2810741 ]\n",
      "[ 40.73082184 -74.00277971]\n",
      "[ 40.92903112 -74.4398702 ]\n",
      "[ 40.62849666 -73.97830735]\n",
      "[ 40.67746059 -73.99712214]\n",
      "[ 40.7668698  -73.98441655]\n",
      "[ 40.67958736 -74.82923698]\n",
      "[ 40.74803041 -74.00385417]\n",
      "[ 40.76009946 -73.98068794]\n",
      "[ 40.59191481 -74.18019557]\n",
      "[ 40.64577674 -73.7802577 ]\n",
      "[ 40.82425056 -74.07757657]\n",
      "[ 40.72552152 -73.99410071]\n",
      "[ 40.60756683 -74.46780436]\n",
      "[ 40.71431896 -73.83217433]\n",
      "[ 40.92803372 -74.1960512 ]\n",
      "[ 40.74453407 -73.98199872]\n",
      "[ 40.76603849 -73.9655797 ]\n",
      "[ 40.70617073 -74.16497845]\n",
      "[ 40.7556011  -73.97205392]\n",
      "[ 40.76017837 -73.99709252]\n",
      "[ 40.75043545 -73.98554693]\n",
      "[ 40.7674333 -73.7198274]\n",
      "[ 40.80127261 -73.93866935]\n",
      "[ 40.78026413 -73.97961089]\n",
      "[ 40.82120778 -73.95255881]\n",
      "[ 40.70871141 -73.5660517 ]\n",
      "[ 40.7755292  -73.94988862]\n",
      "[ 40.79912079 -74.34981315]\n",
      "[ 40.80375115 -73.96478096]\n",
      "[ 40.83945683 -74.72163173]\n",
      "[ 40.72934862 -73.98282905]\n",
      "[ 40.76647712 -73.92043884]\n",
      "[ 40.84826407 -73.85833381]\n",
      "[ 40.74066678 -73.97686685]\n",
      "[ 40.71615868 -73.95700507]\n",
      "[ 40.75079487 -73.9928891 ]\n",
      "[ 40.68216259 -73.97690875]\n",
      "[ 41.19667644 -74.00495172]\n",
      "[ 40.73955078 -74.00577982]\n",
      "[ 40.7737794 -73.9615012]\n",
      "[ 40.67820165 -73.9555336 ]\n",
      "[ 40.7425202  -73.88516411]\n",
      "[ 40.7071496  -74.00866308]\n",
      "[ 40.73836728 -73.98616338]\n",
      "[ 40.78602393 -73.95095956]\n",
      "[ 40.75889484 -73.96483551]\n",
      "[ 40.7194146  -73.99797255]\n",
      "[ 40.74005703 -73.99806306]\n",
      "[ 40.65747735 -73.8914214 ]\n",
      "[ 40.76271334 -73.97318175]\n",
      "[ 40.75277016 -73.93943344]\n",
      "[ 40.79405235 -73.96933784]\n",
      "[ 40.78674205 -73.97460021]\n",
      "[ 40.77367318 -73.87380087]\n",
      "[ 40.4392836  -73.85643238]\n",
      "75 -> SSE: 44.05924139774921\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType()),\n",
    "    StructField(\"hack_license\", StringType()),\n",
    "    StructField(\"pickup_datetime\", StringType()),\n",
    "    StructField(\"dropoff_datetime\", StringType()),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
    "    StructField(\"trip_distance\", FloatType()),\n",
    "    StructField(\"pickup_longitude\", FloatType()),\n",
    "    StructField(\"pickup_latitude\", FloatType()),\n",
    "    StructField(\"dropoff_longitude\", FloatType()),\n",
    "    StructField(\"dropoff_latitude\", FloatType()),\n",
    "    StructField(\"payment_type\", StringType()),\n",
    "    StructField(\"fare_amount\", FloatType()),\n",
    "    StructField(\"surcharge\", FloatType()),\n",
    "    StructField(\"mta_tax\", FloatType()),\n",
    "    StructField(\"tip_amount\", FloatType()),\n",
    "    StructField(\"tolls_amount\", FloatType()),\n",
    "    StructField(\"total_amount\", FloatType())\n",
    "])\n",
    "\n",
    "# Load and parse the data\n",
    "data = spark.read.schema(schema).option(\"header\", \"false\").csv(\"./data/sorted_data.csv\")\n",
    "\n",
    "#Limits to the longitudes and latitudes, everypoint that isn't between these coordinates is considered an outlier\n",
    "lon_min = -74.916578\n",
    "lon_max = -73.120784\n",
    "lat_min = 40.129716\n",
    "lat_max = 41.477183\n",
    "\n",
    "filter_data = data.filter(\n",
    "    (data.pickup_longitude != 0) & \\\n",
    "    (data.pickup_latitude != 0) & \\\n",
    "    (data.dropoff_longitude != 0) & \\\n",
    "    (data.dropoff_latitude != 0) & \\\n",
    "    (data.pickup_longitude <= lon_max) & \\\n",
    "    (data.pickup_longitude >= lon_min) & \\\n",
    "    (data.pickup_latitude >= lat_min) & \\\n",
    "    (data.pickup_latitude <= lat_max) & \\\n",
    "    (data.dropoff_longitude <= lon_max) & \\\n",
    "    (data.dropoff_longitude >= lon_min) & \\\n",
    "    (data.dropoff_latitude >= lat_min) & \\\n",
    "    (data.dropoff_latitude <= lat_max)\n",
    "    )\n",
    "\n",
    "\n",
    "#Define the target columns and output column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [\"pickup_latitude\", \"pickup_longitude\"],\n",
    "    outputCol = \"features\"\n",
    "    )\n",
    "\n",
    "#Transform the data according to the Assembler created above\n",
    "data_prepared = assembler.transform(filter_data)\n",
    "\n",
    "for i in [5, 31, 75]: #find other k values\n",
    "\n",
    "    centroids_file = \"spark_rdd_results/query3/centroids_\"+str(i)+\".txt\"\n",
    "    # Write results to file\n",
    "    f = open(centroids_file,\"w+\")\n",
    "\n",
    "    #Instanciate Kmeans class with the given K value\n",
    "    kmeans = KMeans(k = i)\n",
    "\n",
    "    #Fit the data\n",
    "    model = kmeans.fit(data_prepared)\n",
    "\n",
    "    #Evalute clustering by computing Sum of Square Errors\n",
    "    sum_square_error = model.computeCost(data_prepared)\n",
    "\n",
    "    # To get the prototypes\n",
    "    centers = model.clusterCenters()\n",
    "    print(\"Cluster Centers: \")\n",
    "    for center in centers:\n",
    "        f.write('{:.8f}{}{:.8f}{}'.format(center[0],\",\",center[1],\"\\n\"))\n",
    "\n",
    "    # Close file where centroid positions are stored\n",
    "    f.close()\n",
    "\n",
    "    #The lower the Sum of Square errors is it means the closer the points are to the prototypes, this is equivalent to\n",
    "    #the sum of the square that each person has to walk to nearest taxi stand, so minimizing it would be optimal\n",
    "    print(f\"{i} -> SSE: {sum_square_error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
