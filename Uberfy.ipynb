{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPBD Assignment 2\n",
    "\n",
    "This notebook contains the code developed to implement the propoused solutions to this course assignment\n",
    "\n",
    "Developed by:\n",
    "    * Lucas Fischer, nº54659\n",
    "    * Joana Martins, nº54707\n",
    "    \n",
    "    \n",
    "# IMPORT NOTE:\n",
    "\n",
    "The implemented code runs locally using the sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the dataset\n",
    "\n",
    "The dataset can be obtain by running the command bellow, or by uploading the file manualy to the work directory if the file is already present in your machine\n",
    "\n",
    "# TODO ver como é o link para obter os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o yellow_tripdata_2018-01.csv https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2018-01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating result folders\n",
    "\n",
    "This code removes (if already created) and creates new result folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf spark_rdd_results && mkdir spark_rdd_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up dependencies\n",
    "\n",
    "The first task we must complete is setting up the right dependencies for our solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark import SparkContext\n",
    "import traceback\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import calendar\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('uberfy').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "filename = \"./data/sorted_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions\n",
    "\n",
    "Functions created to help in the development of this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row(line):\n",
    "    \"\"\"\n",
    "        Function that creates a structured tuple representing a row in a RDD\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Rerturns:\n",
    "            A Strcutured tuple with 14 positions\n",
    "    \"\"\"\n",
    "    #Field - Array_position\n",
    "\n",
    "    #pickup_dt - 0      fare_amount - 8\n",
    "    #dropoff_dt - 1     tip_amount - 9\n",
    "    #trip_time - 2      total_amount - 10\n",
    "    #trip_distance - 3  pickup_cell - 11\n",
    "    #pickup_long - 4    dropoff_cell - 12\n",
    "    #pickup_lat - 5     taxi_id = 13\n",
    "    #dropoff_long - 6\n",
    "    #dropoff_lat - 7\n",
    "    \n",
    "    splitted_line = line.split(',')\n",
    "    return (\n",
    "        splitted_line[2], splitted_line[3], int(splitted_line[4]), float(splitted_line[5]), float(splitted_line[6]), \\\n",
    "        float(splitted_line[7]), float(splitted_line[8]), float(splitted_line[9]), float(splitted_line[11]), \\\n",
    "        float(splitted_line[14]), float(splitted_line[16]), estimate_cellid(float(splitted_line[7]), float(splitted_line[6])),\\\n",
    "        estimate_cellid(float(splitted_line[9]), float(splitted_line[8])), splitted_line[0]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_row_df(line):\n",
    "    \"\"\"\n",
    "        Function that creates a Structured Row object representing a Row in a DataFrame\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Returns:\n",
    "            A Row object representing a row in a Dataframe\n",
    "    \"\"\"\n",
    "    #Field - Array_position\n",
    "\n",
    "    #pickup_dt - 0      fare_amount - 8\n",
    "    #dropoff_dt - 1     tip_amount - 9\n",
    "    #trip_time - 2      total_amount - 10\n",
    "    #trip_distance - 3  pickup_cell - 11\n",
    "    #pickup_long - 4    dropoff_cell - 12\n",
    "    #pickup_lat - 5     taxi_id = 13\n",
    "    #dropoff_long - 6\n",
    "    #dropoff_lat - 7\n",
    "    \n",
    "    splitted_line = line.split(',')\n",
    "    return Row(\n",
    "        pickup_dt = splitted_line[2], dropoff_dt = splitted_line[3], trip_time = int(splitted_line[4]), \\\n",
    "        trip_distance = float(splitted_line[5]), pickup_long = float(splitted_line[6]), pickup_lat = float(splitted_line[7]), \\\n",
    "        dropoff_long = float(splitted_line[8]), dropoff_lat = float(splitted_line[9]), fare_amount = float(splitted_line[11]), \\\n",
    "        tip_amount = float(splitted_line[14]), total_amount = float(splitted_line[16]), pickup_cell = estimate_cellid(float(splitted_line[7]), float(splitted_line[6])), \\\n",
    "        dropoff_cell = estimate_cellid(float(splitted_line[9]), float(splitted_line[8])), taxi_id = splitted_line[0]\n",
    "        )   \n",
    "\n",
    "\n",
    "def filter_lines(line):\n",
    "    \"\"\"\n",
    "        Function that filters out empty lines as well as lines that have coordinates as 0.0000 (non relevant points)\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Returns:\n",
    "            True if the line passed this condition, False otherwise\n",
    "    \"\"\"\n",
    "    splitted_line = line.split(',')\n",
    "    return (len(line) > 0) and (float(splitted_line[6]) != 0) and (float(splitted_line[8]) != 0)\n",
    "\n",
    "\n",
    "\n",
    "def estimate_cellid(lat, lon):\n",
    "    \"\"\"\n",
    "        Function that estimates a cell ID given a latitude and longitude based on the coordinates of cell 1.1\n",
    "\n",
    "        Params:\n",
    "            lat - Input latitude for which to find the cellID\n",
    "            lon - Input longitude for which to fin the cellID\n",
    "\n",
    "        Returns:\n",
    "            A String such as 'xxx.xxx' representing the ID of the cell\n",
    "    \"\"\"\n",
    "    x0 = -74.913585 #longitude of cell 1.1\n",
    "    y0 = 41.474937  #latitude of cell 1.1\n",
    "    s = 500 #500 meters\n",
    "\n",
    "    delta_x = 0.005986 / 500.0  #Change in longitude coordinates per meter\n",
    "    delta_y = 0.004491556 /500.0    #Change in latitude coordinates per meter\n",
    "\n",
    "    cell_x = 1 + math.floor((1/2) + (lon - x0)/(s * delta_x))\n",
    "    cell_y = 1 + math.floor((1/2) + (y0 - lat)/(s * delta_y))\n",
    "    \n",
    "    return f\"{cell_x}.{cell_y}\"\n",
    "\n",
    "\n",
    "\n",
    "def create_key_value(structured_tuple):\n",
    "    \"\"\"\n",
    "        Function that from a structured tuple organizes it into a Key-Value formation.\n",
    "        The key is a tuple containing both the weekday and the hour.\n",
    "        The value is a dictionary containing only one item, this dictionary is to be merged on the reducer.\n",
    "\n",
    "        Params:\n",
    "            structured_tuple - A tuple representing a line of the input file\n",
    "\n",
    "        Returns:\n",
    "            A tuple organized into a Key-Value formation\n",
    "    \"\"\"\n",
    "\n",
    "    weekday = convert_to_weekday(structured_tuple[0])\n",
    "    hour = convert_to_hour(structured_tuple[0])\n",
    "    route = f\"{structured_tuple[11]}-{structured_tuple[12]}\"\n",
    "\n",
    "    return ((weekday, hour), {route: 1})\n",
    "\n",
    "\n",
    "\n",
    "def custom_reducer(accum, elem):\n",
    "    \"\"\"\n",
    "        Custom function to be used in reduceByKey.\n",
    "        This function well merge dictionaries counting the number of times each time appears\n",
    "\n",
    "        Params:\n",
    "            accum - An accumulator dictionary\n",
    "            elem - The dictionary of the current iteration\n",
    "\n",
    "        Returns:\n",
    "            The accumulator dictionary updated with information obtained by elem\n",
    "    \"\"\"\n",
    "\n",
    "    #store the only existing item inside elem\n",
    "    key, value = elem.popitem()\n",
    "    \n",
    "    if(key in accum): #If accum already has this key, then update its value\n",
    "        accum[key] += value\n",
    "    else:   #If accum does not have this key, add it\n",
    "        accum[key] = value\n",
    "\n",
    "    return accum\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_weekday(date):\n",
    "    \"\"\"\n",
    "        Function that converts a date to weekday\n",
    "\n",
    "        Params:\n",
    "            date - Unix timestamp formatted date in string form\n",
    "\n",
    "        Returns:\n",
    "            A string with the weekday of the input date\n",
    "    \"\"\"\n",
    "    date_obj = dt.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return (calendar.day_name[date_obj.weekday()]).lower()\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_hour(date):\n",
    "    \"\"\"\n",
    "        Function that gets the hour from a date\n",
    "\n",
    "        Params:\n",
    "            date - Unix timestamp formatted date in string form\n",
    "\n",
    "        Returns:\n",
    "            The hour portion of the input date\n",
    "    \"\"\"\n",
    "    return date[11:13]\n",
    "\n",
    "\n",
    "\n",
    "def filter_outliers(structured_tuple):\n",
    "    \"\"\"\n",
    "        Function that filters out outlier cells. Cells whos ID is above 300 are considered outliers since\n",
    "        the grid only extends to cell 300.300\n",
    "\n",
    "        Params:\n",
    "            structured_tuple - A tuple containing information of a line in the RDD\n",
    "\n",
    "        Returns:\n",
    "            True if there are no outlier cells in the input tuple, False otherwise\n",
    "    \"\"\"\n",
    "    pickup_cell_x , pickup_cell_y = structured_tuple[11].split(\".\")\n",
    "    dropoff_cell_x , dropoff_cell_y = structured_tuple[12].split(\".\")\n",
    "    return (float(pickup_cell_x) <= 300) and (float(pickup_cell_y) <= 300) and (float(dropoff_cell_x) <= 300) and (float(dropoff_cell_y) <= 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 1\n",
    "\n",
    "Falar aqui sobre a primeira query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        \n",
    "    #timestamp to mesure the time taken\n",
    "    time_before = dt.now()\n",
    "\n",
    "    #read csv file (change this to the full dataset instead of just the sample)\n",
    "    raw_data = sc.textFile(filename)\n",
    "\n",
    "    #Filtering out non empty lines and lines that have a pick up or drop off coordinates as 0\n",
    "    non_empty_lines = raw_data.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    #Shapping the rdd rows\n",
    "    fields = non_empty_lines.map(lambda line : create_row(line))\n",
    "\n",
    "    # Filter out rows that have Cell ID's with 300 in them. They are considered as outliers (stated in http://debs.org/debs-2015-grand-challenge-taxi-trips/)\n",
    "    filtered_rdd = fields.filter(lambda row: filter_outliers(row))\n",
    "\n",
    "    # ((weekday, hour), {route})\n",
    "    organized_lines = filtered_rdd.map(lambda line : create_key_value(line))\n",
    "\n",
    "    #Group all values by its key, reducing them acording to custom_reducer\n",
    "    grouped = organized_lines.reduceByKey(lambda accum, elem: custom_reducer(accum, elem))\n",
    "\n",
    "    #Sort descendingly the dictionaries present in the values and take only the first 10 elements\n",
    "    top_routes = grouped.mapValues(lambda route_dict: sorted(route_dict, key = route_dict.get, reverse = True)[:10])\n",
    "\n",
    "    #Store the retrieved results\n",
    "    top_routes.saveAsTextFile(\"spark_rdd_results/query1\")\n",
    "\n",
    "    for a in top_routes.take(2):\n",
    "        print(a)\n",
    "\n",
    "    time_after = dt.now()\n",
    "    seconds = (time_after - time_before).total_seconds()\n",
    "    print(\"Execution time {} seconds\".format(seconds))\n",
    "\n",
    "\n",
    "    # sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    # sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 2\n",
    "\n",
    "Falar aqui sobre a query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    #timestamp to mesure the time taken\n",
    "    time_before = dt.now()\n",
    "\n",
    "    # convert_to_weekday_udf = udf(lambda pickup_date: convert_to_weekday(pickup_date), StringType())\n",
    "    convert_to_weekday_udf = udf(lambda pickup_date: convert_to_weekday(pickup_date), StringType())\n",
    "    convert_to_hour_udf = udf(lambda pickup_date: pickup_date[11:13], StringType())\n",
    "\n",
    "    #read csv file (change this to the full dataset instead of just the sample)\n",
    "    raw_data = sc.textFile(filename)\n",
    "\n",
    "    #Filtering out non empty lines and lines that have a pick up or drop off coordinates as 0\n",
    "    non_empty_lines = raw_data.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    #Shapping the rdd rows\n",
    "    fields = non_empty_lines.map(lambda line : create_row_df(line))\n",
    "\n",
    "    #Creating DataFrame\n",
    "    lines_df = spark.createDataFrame(fields)\n",
    "\n",
    "    # Filter out rows that have Cell ID's with 300 in them. They are considered as outliers (stated in http://debs.org/debs-2015-grand-challenge-taxi-trips/)\n",
    "    filtered_df = lines_df.filter(~((lines_df.pickup_cell.rlike(\"3\\d\\d\")) | (lines_df.dropoff_cell.rlike(\"3\\d\\d\"))))\n",
    "\n",
    "    # Get the dropoffs of the last 15 minutes for each cell\n",
    "    # get the average of the fare\n",
    "    profit_by_area_15min = filtered_df \\\n",
    "        .groupBy(window(\"dropoff_dt\", \"900 seconds\"), convert_to_weekday_udf(\"pickup_dt\").alias(\"weekday\"), convert_to_hour_udf(\"pickup_dt\").alias(\"hour\"), \"pickup_cell\") \\\n",
    "        .agg(avg(filtered_df.fare_amount + filtered_df.tip_amount).alias(\"median_fare\")) \\\n",
    "        .orderBy(\"median_fare\", ascending = False) \\\n",
    "        .select(\"weekday\", \"hour\", \"pickup_cell\")\n",
    "\n",
    "\n",
    "    # empty_taxis = filtered_df \\\n",
    "    #     .groupBy(window(\"dropoff_dt\", \"900 seconds\"), \"dropoff_cell\") \\\n",
    "    #     .agg(countDistinct(\"taxi_id\").alias(\"empty_taxis\")) \\\n",
    "    #     .select(\"dropoff_cell\", \"empty_taxis\")\n",
    "\n",
    "    profit_by_area_15min.show(2)\n",
    "    \n",
    "    profit_by_area_15min.rdd.map(lambda row: ((row.weekday, row.hour), row.pickup_cell)).saveAsTextFile(\"spark_rdd_results/query2\")\n",
    "    \n",
    "    time_after = dt.now()\n",
    "    seconds = (time_after - time_before).total_seconds()\n",
    "    print(\"Execution time {} seconds\".format(seconds))\n",
    "\n",
    "    # sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    # sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 3\n",
    "\n",
    "Falar aqui sobre a query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"medallion\", StringType()),\n",
    "        StructField(\"hack_license\", StringType()),\n",
    "        StructField(\"pickup_datetime\", StringType()),\n",
    "        StructField(\"dropoff_datetime\", StringType()),\n",
    "        StructField(\"trip_time_in_secs\", IntegerType()),\n",
    "        StructField(\"trip_distance\", FloatType()),\n",
    "        StructField(\"pickup_longitude\", FloatType()),\n",
    "        StructField(\"pickup_latitude\", FloatType()),\n",
    "        StructField(\"dropoff_longitude\", FloatType()),\n",
    "        StructField(\"dropoff_latitude\", FloatType()),\n",
    "        StructField(\"payment_type\", StringType()),\n",
    "        StructField(\"fare_amount\", FloatType()),\n",
    "        StructField(\"surcharge\", FloatType()),\n",
    "        StructField(\"mta_tax\", FloatType()),\n",
    "        StructField(\"tip_amount\", FloatType()),\n",
    "        StructField(\"tolls_amount\", FloatType()),\n",
    "        StructField(\"total_amount\", FloatType())\n",
    "    ])\n",
    "\n",
    "# Load and parse the data\n",
    "data = spark.read.schema(schema).option(\"header\", \"false\").csv(\"./data/sorted_data.csv\")\n",
    "\n",
    "#Define the target columns and output column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [\"pickup_latitude\", \"pickup_longitude\"],\n",
    "    outputCol = \"features\"\n",
    "    )\n",
    "\n",
    "#Transform the data according to the Assembler created above\n",
    "data_prepared = assembler.transform(data)\n",
    "\n",
    "#Class used to evaluate the clusters\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "\n",
    "for i in [5, 31, 75]: #find other k values\n",
    "    \n",
    "    #Instanciate Kmeans class with the given K value\n",
    "    kmeans = KMeans(k = i)\n",
    "\n",
    "    #Fit the data\n",
    "    model = kmeans.fit(data_prepared)\n",
    "\n",
    "    #Make predictions on fitted data\n",
    "    predictions = model.transform(data_prepared)\n",
    "\n",
    "    #Evaluate clustering by computing Silhouettes score\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "    #TODO get position of prototype with best silhouette score (probably going to be the last iteration)\n",
    "\n",
    "    #The closer silhouette score is to 1 means the tighter the points of the same cluster are, and the farther they are from other clusters\n",
    "    #This is optimal because it means that points will all be close to just one taxi stand (saving unecessary money to create another one)\n",
    "    print(f\"{i} -> {silhouette}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
