{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPBD Assignment 2\n",
    "\n",
    "This notebook contains the code developed to implement the propoused solutions to this course assignment\n",
    "\n",
    "Developed by:\n",
    "    * Lucas Fischer, nº54659\n",
    "    * Joana Martins, nº54707\n",
    "    \n",
    "    \n",
    "# IMPORT NOTE:\n",
    "\n",
    "The implemented code runs locally using the sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset\n",
    "\n",
    "The data can be obtained from https://drive.google.com/file/d/0B0TBL8JNn3JgTGNJTEJaQmFMbk0/view and should be placed in a root of the assignment directory. **(Curling the dataset was not achieved in this assignment, so this step mustbe done manualy).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating result folders\n",
    "\n",
    "This code removes (if already created) and creates new result folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf spark_rdd_results && mkdir spark_rdd_results\n",
    "!cd spark_rdd_results && mkdir query3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up dependencies\n",
    "\n",
    "The first task we must complete is setting up the right dependencies for our solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark import SparkContext\n",
    "import traceback\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "import calendar\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('uberfy').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "filename = \"sorted_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions\n",
    "\n",
    "Functions created to help in the development of this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_row(line):\n",
    "    \"\"\"\n",
    "        Function that creates a structured tuple representing a row in a RDD\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Rerturns:\n",
    "            A Structured tuple with 14 positions\n",
    "    \"\"\"\n",
    "    #Field - Array_position\n",
    "\n",
    "    #pickup_dt - 0      fare_amount - 8\n",
    "    #dropoff_dt - 1     tip_amount - 9\n",
    "    #trip_time - 2      total_amount - 10\n",
    "    #trip_distance - 3  pickup_cell - 11\n",
    "    #pickup_long - 4    dropoff_cell - 12\n",
    "    #pickup_lat - 5     taxi_id = 13\n",
    "    #dropoff_long - 6\n",
    "    #dropoff_lat - 7\n",
    "    \n",
    "    splitted_line = line.split(',')\n",
    "    return (\n",
    "        splitted_line[2], splitted_line[3], int(splitted_line[4]), float(splitted_line[5]), float(splitted_line[6]), \\\n",
    "        float(splitted_line[7]), float(splitted_line[8]), float(splitted_line[9]), float(splitted_line[11]), \\\n",
    "        float(splitted_line[14]), float(splitted_line[16]), estimate_cellid(float(splitted_line[7]), float(splitted_line[6])),\\\n",
    "        estimate_cellid(float(splitted_line[9]), float(splitted_line[8])), splitted_line[0]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_row_df(line):\n",
    "    \"\"\"\n",
    "        Function that creates a Structured Row object representing a Row in a DataFrame\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Returns:\n",
    "            A Row object representing a row in a Dataframe\n",
    "    \"\"\"\n",
    "    #Field - Array_position\n",
    "\n",
    "    #pickup_dt - 0      fare_amount - 8\n",
    "    #dropoff_dt - 1     tip_amount - 9\n",
    "    #trip_time - 2      total_amount - 10\n",
    "    #trip_distance - 3  pickup_cell - 11\n",
    "    #pickup_long - 4    dropoff_cell - 12\n",
    "    #pickup_lat - 5     taxi_id = 13\n",
    "    #dropoff_long - 6\n",
    "    #dropoff_lat - 7\n",
    "    \n",
    "    splitted_line = line.split(',')\n",
    "    return Row(\n",
    "        pickup_dt = splitted_line[2], dropoff_dt = splitted_line[3], trip_time = int(splitted_line[4]), \\\n",
    "        trip_distance = float(splitted_line[5]), pickup_long = float(splitted_line[6]), pickup_lat = float(splitted_line[7]), \\\n",
    "        dropoff_long = float(splitted_line[8]), dropoff_lat = float(splitted_line[9]), fare_amount = float(splitted_line[11]), \\\n",
    "        tip_amount = float(splitted_line[14]), total_amount = float(splitted_line[16]), pickup_cell = estimate_cellid(float(splitted_line[7]), float(splitted_line[6])), \\\n",
    "        dropoff_cell = estimate_cellid(float(splitted_line[9]), float(splitted_line[8])), taxi_id = splitted_line[0]\n",
    "        )   \n",
    "\n",
    "\n",
    "def filter_lines(line):\n",
    "    \"\"\"\n",
    "        Function that filters out empty lines as well as lines that have coordinates as 0.0000 (non relevant points)\n",
    "\n",
    "        Params:\n",
    "            line - A line from the input file\n",
    "\n",
    "        Returns:\n",
    "            True if the line passed this condition, False otherwise\n",
    "    \"\"\"\n",
    "    splitted_line = line.split(',')\n",
    "    \n",
    "    #Limits of the grid. Every point that is not between these coordinates will be considered as an outlier\n",
    "    lon_min = -74.916578\n",
    "    lon_max = -73.120784\n",
    "    lat_min = 40.129716\n",
    "    lat_max = 41.477183\n",
    "\n",
    "    return (\n",
    "        len(line) > 0) and \\\n",
    "        (float(splitted_line[6]) != 0) and \\\n",
    "        (float(splitted_line[8]) != 0 and \\\n",
    "        (float(splitted_line[6]) >= lon_min) and \\\n",
    "        (float(splitted_line[6]) <= lon_max) and \\\n",
    "        (float(splitted_line[7]) >= lat_min) and \\\n",
    "        (float(splitted_line[7]) <= lat_max) and \\\n",
    "        (float(splitted_line[8]) >= lon_min) and \\\n",
    "        (float(splitted_line[8]) <= lon_max) and \\\n",
    "        (float(splitted_line[9]) >= lat_min) and \\\n",
    "        (float(splitted_line[9]) <= lat_max)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def estimate_cellid(lat, lon):\n",
    "    \"\"\"\n",
    "        Function that estimates a cell ID given a latitude and longitude based on the coordinates of cell 1.1\n",
    "\n",
    "        Params:\n",
    "            lat - Input latitude for which to find the cellID\n",
    "            lon - Input longitude for which to fin the cellID\n",
    "\n",
    "        Returns:\n",
    "            A String such as 'xxx.xxx' representing the ID of the cell\n",
    "    \"\"\"\n",
    "    x0 = -74.913585 #longitude of cell 1.1\n",
    "    y0 = 41.474937  #latitude of cell 1.1\n",
    "    s = 500 #500 meters\n",
    "\n",
    "    delta_x = 0.005986 / 500.0  #Change in longitude coordinates per meter\n",
    "    delta_y = 0.004491556 /500.0    #Change in latitude coordinates per meter\n",
    "\n",
    "    cell_x = 1 + math.floor((1/2) + (lon - x0)/(s * delta_x))\n",
    "    cell_y = 1 + math.floor((1/2) + (y0 - lat)/(s * delta_y))\n",
    "    \n",
    "    return f\"{cell_x}.{cell_y}\"\n",
    "\n",
    "\n",
    "\n",
    "def create_key_value(structured_tuple):\n",
    "    \"\"\"\n",
    "        Function that from a structured tuple organizes it into a Key-Value formation.\n",
    "        The key is a tuple containing both the weekday and the hour.\n",
    "        The value is a dictionary containing only one item, this dictionary is to be merged on the reducer.\n",
    "\n",
    "        Params:\n",
    "            structured_tuple - A tuple representing a line of the input file\n",
    "\n",
    "        Returns:\n",
    "            A tuple organized into a Key-Value formation\n",
    "    \"\"\"\n",
    "\n",
    "    weekday = convert_to_weekday(structured_tuple[0])\n",
    "    hour = convert_to_hour(structured_tuple[0])\n",
    "    route = f\"{structured_tuple[11]}-{structured_tuple[12]}\"\n",
    "\n",
    "    return ((weekday, hour), {route: 1})\n",
    "\n",
    "\n",
    "\n",
    "def custom_reducer(accum, elem):\n",
    "    \"\"\"\n",
    "        Custom function to be used in reduceByKey.\n",
    "        This function well merge dictionaries counting the number of times each time appears\n",
    "\n",
    "        Params:\n",
    "            accum - An accumulator dictionary\n",
    "            elem - The dictionary of the current iteration\n",
    "\n",
    "        Returns:\n",
    "            The accumulator dictionary updated with information obtained by elem\n",
    "    \"\"\"\n",
    "\n",
    "    #store the only existing item inside elem\n",
    "    key, value = elem.popitem()\n",
    "    \n",
    "    if(key in accum): #If accum already has this key, then update its value\n",
    "        accum[key] += value\n",
    "    else:   #If accum does not have this key, add it\n",
    "        accum[key] = value\n",
    "\n",
    "    return accum\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_weekday(date):\n",
    "    \"\"\"\n",
    "        Function that converts a date to weekday\n",
    "\n",
    "        Params:\n",
    "            date - Unix timestamp formatted date in string form\n",
    "\n",
    "        Returns:\n",
    "            A string with the weekday of the input date\n",
    "    \"\"\"\n",
    "    date_obj = dt.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return (calendar.day_name[date_obj.weekday()]).lower()\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_hour(date):\n",
    "    \"\"\"\n",
    "        Function that gets the hour from a date\n",
    "\n",
    "        Params:\n",
    "            date - Unix timestamp formatted date in string form\n",
    "\n",
    "        Returns:\n",
    "            The hour portion of the input date\n",
    "    \"\"\"\n",
    "    return date[11:13]\n",
    "\n",
    "\n",
    "def get_last_mins(structured_tuple, last_15_min = True):\n",
    "    \"\"\"\n",
    "        Function that filters lines that do not represent the last 15 minutes of an hour\n",
    "\n",
    "        Params:\n",
    "            structured_tuple - A tuple representing a line of the input file\n",
    "            last_15_min - Flag to specify if the line should be inside the last 15 minutes or inside the last 30 minutes of an hour (defaults to 15 min)\n",
    "\n",
    "        Returns:\n",
    "            True if this row has a dropoff hour inside the last x minutes of an hour, False otherwise\n",
    "    \"\"\"\n",
    "    input_dropoff_mins = int(structured_tuple[1][14:16]) #Get the minutes portion of the input datetime\n",
    "\n",
    "    if(last_15_min):\n",
    "\n",
    "        #Returns true if the input line is inside the last 15 minutes of an hour\n",
    "        return input_dropoff_mins >= 45\n",
    "\n",
    "    else:\n",
    "\n",
    "        #Returns true if the input line is inside the last 30 minutes of an hour\n",
    "        return input_dropoff_mins >= 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 1\n",
    "\n",
    "In this first query the objective was to obtain an index where it would be possible to check the 10 most frequent taxi routes on any given weekday and hour.\n",
    "\n",
    "To do so the group first had to map the latitude and longitude coordinates of every event into a 500m x 500m cell in order to represent a small area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    #timestamp to mesure the time taken\n",
    "    time_before = dt.now()\n",
    "\n",
    "    #read csv file (change this to the full dataset instead of just the sample)\n",
    "    raw_data = sc.textFile(filename)\n",
    "\n",
    "    #Filtering out non empty lines and lines that have a pick up or drop off coordinates as 0\n",
    "    #Also filtering lines that have coordinates that would be mapped to cells with ID greater than 300 and lower de 1\n",
    "    #These lines are considerer outliers (stated in http://debs.org/debs-2015-grand-challenge-taxi-trips/)\n",
    "    non_empty_lines = raw_data.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    #Shaping the rdd rows\n",
    "    fields = non_empty_lines.map(lambda line : create_row(line))\n",
    "\n",
    "    # ((weekday, hour), {route})\n",
    "    organized_lines = fields.map(lambda line : create_key_value(line))\n",
    "\n",
    "    #Group all values by its key, reducing them acording to custom_reducer\n",
    "    grouped = organized_lines.reduceByKey(lambda accum, elem: custom_reducer(accum, elem))\n",
    "\n",
    "    #Sort descendingly the dictionaries present in the values and take only the first 10 elements\n",
    "    top_routes = grouped.mapValues(lambda route_dict: sorted(route_dict, key = route_dict.get, reverse = True)[:10])\n",
    "\n",
    "    #Store the retrieved results\n",
    "    top_routes.saveAsTextFile(\"spark_rdd_results/query1\")\n",
    "\n",
    "    for a in top_routes.take(2):\n",
    "        print(a)\n",
    "\n",
    "    time_after = dt.now()\n",
    "    seconds = (time_after - time_before).total_seconds()\n",
    "    print(\"Execution time {} seconds\".format(seconds))\n",
    "\n",
    "    # sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    # sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full index can be obtained in the spark_rdd_results/query1 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat spark_rdd_results/query1/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 2\n",
    "\n",
    "Another question of interesent is \"What are the most profitable areas?\". In order to obtain a conclusion the group started by calculating the most profitable areas in a given weekday and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    #timestamp to measure the time taken\n",
    "    time_before = dt.now()\n",
    "\n",
    "    #read csv file (change this to the full dataset instead of just the sample)\n",
    "    raw_data = sc.textFile(filename)\n",
    "\n",
    "    #Filtering out non-empty lines and lines that have a pick up or drop off coordinates as 0\n",
    "    non_empty_lines = raw_data.filter(lambda line: filter_lines(line))\n",
    "\n",
    "    #Shaping the rdd rows\n",
    "    fields = non_empty_lines.map(lambda line : create_row(line))\n",
    "\n",
    "    #Filter every line that is not inside the last 30 minutes of a drop-off hour\n",
    "    last_30_mins = fields.filter(lambda line: get_last_mins(line, last_15_min = False))\n",
    "\n",
    "    #Filter every line that is not inside the last 15 minutes of a drop-off hour\n",
    "    last_15_mins = last_30_mins.filter(lambda line: get_last_mins(line))\n",
    "\n",
    "    #Get the number of empty taxis for each area and hour of every weekday\n",
    "    empty_taxis = last_30_mins \\\n",
    "    .map(lambda line: ((convert_to_weekday(line[1]), convert_to_hour(line[1]), line[12]), 1)) \\\n",
    "    .reduceByKey(lambda accum , elem: accum + elem)\n",
    "\n",
    "    #First organize lines into ((weekday, hour), (fare amount + tip amount, 1))\n",
    "    #Then reduce every value to the same key by summing the corresponding tuple elements\n",
    "    #Then divide the first element in the value tuple by the second one\n",
    "    profitability = last_15_mins \\\n",
    "    .map(lambda line: ((convert_to_weekday(line[1]), convert_to_hour(line[1]), line[12]), (line[8] + line[9], 1))) \\\n",
    "    .reduceByKey(lambda accum, elem : (accum[0] + elem[0], accum[1] + elem[1])) \\\n",
    "    .mapValues(lambda tup : tup[0] / tup[1])\n",
    "\n",
    "    joined = empty_taxis.join(profitability).mapValues(lambda tup: tup[1] / tup[0])\n",
    "\n",
    "    #First organize lines into ((weekday, hour, dropoff_cell), [dropoff_cell, profitability])\n",
    "    #Then reduce every value to the same key by appending the lists\n",
    "    #Then sort descendingly the list looking at position one in the value tuple (profitability)\n",
    "    #and take lines from 10 highest values        \n",
    "    #Then retrieve only the dropoff_cells for each key (weekday, hour)\n",
    "    most_profitable_areas = joined \\\n",
    "    .map(lambda tup: ((tup[0][0], tup[0][1]), [(tup[0][2], tup[1])]))  \\\n",
    "    .reduceByKey(lambda accum, elem : accum + elem) \\\n",
    "    .mapValues(lambda tup_list: sorted(tup_list, key = lambda tup: -tup[1])[:10]) \\\n",
    "    .mapValues(lambda sorted_list: [tup[0] for tup in sorted_list])\n",
    "\n",
    "    for a in most_profitable_areas.take(2):\n",
    "        print(a)\n",
    "\n",
    "    #Save results\n",
    "    most_profitable_areas.saveAsTextFile(\"spark_rdd_results/query2\")\n",
    "\n",
    "    time_after = dt.now()\n",
    "    seconds = (time_after - time_before).total_seconds()\n",
    "    print(\"Execution time {} seconds\".format(seconds))\n",
    "\n",
    "    # sc.stop()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "    # sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full index can be obtained under the spark_rdd_results/query2 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat spark_rdd_results/query2/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 3\n",
    "\n",
    "In this last challenge the objetive was to evaluate the best possible locations to place a taxi stand. Naturaly a taxi stand should be placed where there's a larger influx of people in order to maximize the profit for the taxi company and minimize the time a passager has to travel to a taxi stand.\n",
    "\n",
    "In this solution the group used Kmeans clustering algorithm representing the prototypes of the clusters as the taxi stand locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType()),\n",
    "    StructField(\"hack_license\", StringType()),\n",
    "    StructField(\"pickup_datetime\", StringType()),\n",
    "    StructField(\"dropoff_datetime\", StringType()),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
    "    StructField(\"trip_distance\", FloatType()),\n",
    "    StructField(\"pickup_longitude\", FloatType()),\n",
    "    StructField(\"pickup_latitude\", FloatType()),\n",
    "    StructField(\"dropoff_longitude\", FloatType()),\n",
    "    StructField(\"dropoff_latitude\", FloatType()),\n",
    "    StructField(\"payment_type\", StringType()),\n",
    "    StructField(\"fare_amount\", FloatType()),\n",
    "    StructField(\"surcharge\", FloatType()),\n",
    "    StructField(\"mta_tax\", FloatType()),\n",
    "    StructField(\"tip_amount\", FloatType()),\n",
    "    StructField(\"tolls_amount\", FloatType()),\n",
    "    StructField(\"total_amount\", FloatType())\n",
    "])\n",
    "\n",
    "# Load and parse the data\n",
    "data = spark.read.schema(schema).option(\"header\", \"false\").csv(\"./data/sorted_data.csv\")\n",
    "\n",
    "#Limits to the longitudes and latitudes, everypoint that isn't between these coordinates is considered an outlier\n",
    "lon_min = -74.916578\n",
    "lon_max = -73.120784\n",
    "lat_min = 40.129716\n",
    "lat_max = 41.477183\n",
    "\n",
    "filter_data = data.filter(\n",
    "    (data.pickup_longitude != 0) & \\\n",
    "    (data.pickup_latitude != 0) & \\\n",
    "    (data.dropoff_longitude != 0) & \\\n",
    "    (data.dropoff_latitude != 0) & \\\n",
    "    (data.pickup_longitude <= lon_max) & \\\n",
    "    (data.pickup_longitude >= lon_min) & \\\n",
    "    (data.pickup_latitude >= lat_min) & \\\n",
    "    (data.pickup_latitude <= lat_max) & \\\n",
    "    (data.dropoff_longitude <= lon_max) & \\\n",
    "    (data.dropoff_longitude >= lon_min) & \\\n",
    "    (data.dropoff_latitude >= lat_min) & \\\n",
    "    (data.dropoff_latitude <= lat_max)\n",
    "    )\n",
    "\n",
    "\n",
    "#Define the target columns and output column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [\"pickup_latitude\", \"pickup_longitude\"],\n",
    "    outputCol = \"features\"\n",
    "    )\n",
    "\n",
    "#Transform the data according to the Assembler created above\n",
    "data_prepared = assembler.transform(filter_data)\n",
    "\n",
    "for i in [5, 31, 75]: #find other k values\n",
    "\n",
    "    centroids_file = \"spark_rdd_results/query3/centroids_\"+str(i)+\".txt\"\n",
    "    # Write results to file\n",
    "    f = open(centroids_file,\"w+\")\n",
    "\n",
    "    #Instanciate Kmeans class with the given K value\n",
    "    kmeans = KMeans(k = i)\n",
    "\n",
    "    #Fit the data\n",
    "    model = kmeans.fit(data_prepared)\n",
    "\n",
    "    #Evalute clustering by computing Sum of Square Errors\n",
    "    sum_square_error = model.computeCost(data_prepared)\n",
    "\n",
    "    # To get the prototypes\n",
    "    centers = model.clusterCenters()\n",
    "    print(\"Cluster Centers: \")\n",
    "    for center in centers:\n",
    "        f.write('{:.8f}{}{:.8f}{}'.format(center[0],\",\",center[1],\"\\n\"))\n",
    "\n",
    "    # Close file where centroid positions are stored\n",
    "    f.close()\n",
    "\n",
    "    #The lower the Sum of Square errors is it means the closer the points are to the prototypes, this is equivalent to\n",
    "    #the sum of the square that each person has to walk to nearest taxi stand, so minimizing it would be optimal\n",
    "    print(f\"{i} -> SSE: {sum_square_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locations of the proposed taxi stands can be viewed under the spark_rdd_results/query3 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat spark_rdd_results/query3/centroids_*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
